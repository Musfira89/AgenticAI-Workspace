{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains with the help of Langgraph\n",
    "\n",
    "1. How to use chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: LLMModel\n",
      "\n",
      "Please tell me how can i help?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Musfira\n",
      "\n",
      "I want to learn coding\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "from pprint import pprint\n",
    "\n",
    "messages=[AIMessage(content=f\"Please tell me how can i help?\",name=\"LLMModel\")]\n",
    "messages.append(HumanMessage(content=f\"I want to learn coding\",name=\"Musfira\"))\n",
    "\n",
    "for message in messages:\n",
    "    message.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Chat model: use sequence of messages as a input for llm models (openAI, Groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 448,\n",
       "  'prompt_tokens': 53,\n",
       "  'total_tokens': 501,\n",
       "  'completion_time': 0.534886563,\n",
       "  'prompt_time': 0.002499357,\n",
       "  'queue_time': 0.050926273,\n",
       "  'total_time': 0.53738592},\n",
       " 'model_name': 'llama-3.1-8b-instant',\n",
       " 'system_fingerprint': 'fp_ab04adca7d',\n",
       " 'service_tier': 'on_demand',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# llm=ChatGroq(model=\"gemma2-9b-it\")\n",
    "llm=ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "result=llm.invoke(messages)\n",
    "result.response_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tools\n",
    "tools can be integrated with the llm models to interact with external systems.\n",
    "\n",
    "4. Router\n",
    "because of this router, you know, you'll be able to see now nodes that are there with some kind of functionalities which uses LLM, how they can be specifically converted into an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a:int,b:int)-> int:\n",
    "    \"\"\" Add a and b\n",
    "    Args:\n",
    "        a (int): first int\n",
    "        b (int): second int\n",
    "    Returns:\n",
    "        int   \n",
    "        \n",
    "    \"\"\"\n",
    "    return a+b\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B789CAA870>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B789CAB020>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'zpc48t92y', 'function': {'arguments': '{\"a\":2,\"b\":2}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 272, 'total_tokens': 290, 'completion_time': 0.032506647, 'prompt_time': 0.014912736, 'queue_time': 0.051210884, 'total_time': 0.047419383}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--aed0dc92-bf5d-43f9-89fa-0f28c95ce88a-0', tool_calls=[{'name': 'add', 'args': {'a': 2, 'b': 2}, 'id': 'zpc48t92y', 'type': 'tool_call'}], usage_metadata={'input_tokens': 272, 'output_tokens': 18, 'total_tokens': 290})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Binding tool with llm\n",
    "\n",
    "llm_with_tools=llm.bind_tools([add])\n",
    "llm_with_tools.invoke([HumanMessage(content=f\"Hey what is 2+2?\",name=\"Musfira\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from typing import  Annotated\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages:Annotated[list[AnyMessage],add_messages]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
